import os
import re
import json

import joblib
import numpy as np
import pandas as pd
import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity
import pymorphy2

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
os.makedirs("model", exist_ok=True)

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
dfs = []
for i in range(1, 7):
    df = pd.read_csv(f"data/{i}.csv")
    dfs.append(df)

data = pd.concat(dfs, ignore_index=True)

# 2. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
morph = pymorphy2.MorphAnalyzer()

def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"[^–∞-—èa-z0-9\s]", " ", text)  # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ –±—É–∫–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r"\s+", " ", text)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã

    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    words = text.split()
    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]

    return " ".join(lemmatized_words)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
for col in ['doc_text', 'image2text', 'speech2text']:
    data[col] = data[col].apply(clean_text)

# 3. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
data['full_text'] = data['doc_text'] + " " + data['image2text'] + " " + data['speech2text']

# 4. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ BERT –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')
model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased')

# 5. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞
def get_embeddings(texts):
    embeddings = []
    for text in texts:
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            output = model(**inputs)
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è [CLS] —Ç–æ–∫–µ–Ω–∞ (–ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω)
        embeddings.append(output.last_hidden_state.mean(dim=1).squeeze().numpy())
    return np.array(embeddings)

# 6. –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
doc_texts = data['full_text'].tolist()
doc_embeddings = get_embeddings(doc_texts)

# 7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
test_queries = [
    {"query": "–ø–æ–≥–æ–¥–∞ –Ω–∞ –∑–∞–≤—Ç—Ä–∞", "relevant_ids": [0, 3]},
    {"query": "–Ω–æ–≤–æ—Å—Ç–∏ —Å–ø–æ—Ä—Ç–∞", "relevant_ids": [5, 12]},
    {"query": "—Ä–∞–∑–≥–æ–≤–æ—Ä –æ –∑–¥–æ—Ä–æ–≤—å–µ", "relevant_ids": [8]},
]

# 8. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ–∏—Å–∫–∞
from sklearn.metrics import f1_score

def compute_search_metrics(queries, embeddings, texts, model, k=5):
    precision_list = []
    recall_list = []
    reciprocal_ranks = []
    f1_list = []

    for item in queries:
        query = item["query"]
        relevant_ids = item["relevant_ids"]

        query_clean = clean_text(query)
        query_emb = get_embeddings([query_clean])[0]
        sims = cosine_similarity([query_emb], embeddings)[0]

        top_k_idx = np.argsort(sims)[::-1][:k]

        hits = sum(1 for idx in top_k_idx if idx in relevant_ids)
        precision_at_k = hits / k
        recall_at_k = hits / len(relevant_ids)

        precision_list.append(precision_at_k)
        recall_list.append(recall_at_k)

        rr = 0
        for rank, idx in enumerate(top_k_idx, start=1):
            if idx in relevant_ids:
                rr = 1 / rank
                break
        reciprocal_ranks.append(rr)

        f1 = f1_score([1 if idx in relevant_ids else 0 for idx in top_k_idx], [1] * k)
        f1_list.append(f1)

    metrics = {
        "precision@5": round(np.mean(precision_list), 3),
        "recall@5": round(np.mean(recall_list), 3),
        "mrr": round(np.mean(reciprocal_ranks), 3),
        "f1_score": round(np.mean(f1_list), 3),
        "test_size": len(queries)
    }
    return metrics

search_metrics = compute_search_metrics(test_queries, doc_embeddings, doc_texts, model)

# 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç—Ä–∏–∫
np.save("model/doc_embeddings.npy", doc_embeddings)
joblib.dump(model, "model/model.pkl")
data[['full_text']].to_csv("model/doc_texts.csv", index=False)

with open("model/search_metrics.json", "w", encoding="utf-8") as f:
    json.dump(search_metrics, f, ensure_ascii=False, indent=2)

print(f"‚úÖ –ú–æ–¥–µ–ª—å, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'model/'")
print("üìä Search Metrics:", search_metrics)
