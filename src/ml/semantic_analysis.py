import os
import re
import joblib
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer, util

# –û—Ç–∫–ª—é—á–∞–µ–º TensorFlow (—Ä–∞–±–æ—Ç–∞–µ–º —á–µ—Ä–µ–∑ PyTorch)
os.environ["USE_TF"] = "0"

# === –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===
model_path = 'model/semantic_search_model'
if os.path.exists(model_path):
    ST_model = SentenceTransformer(model_path)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ")
else:
    ST_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    os.makedirs('model', exist_ok=True)
    ST_model.save(model_path)
    print("üì• –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")


# === –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ===
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# === –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ ===
def semantic_search_in_document(document, query, model):
    doc_cleaned = clean_text(document)
    query_cleaned = clean_text(query)

    doc_words = doc_cleaned.split()
    query_words = query_cleaned.split()

    if len(query_words) > 2:
        return {"document": document, "distance": 1.0, "positions": "0-0", "matched_word": ""}

    doc_embedding = model.encode(doc_cleaned, convert_to_tensor=True)
    query_embedding = model.encode(query_cleaned, convert_to_tensor=True)

    cosine_similarity = util.cos_sim(query_embedding, doc_embedding)[0]
    cosine_distance = 1.0 - cosine_similarity

    if cosine_distance > 0.5:
        return {"document": document, "distance": float(cosine_distance), "positions": "0-0", "matched_word": ""}

    doc_words_embeddings = model.encode(doc_words, convert_to_tensor=True)
    word_cosine_scores = util.cos_sim(query_embedding, doc_words_embeddings)[0]
    best_word_idx = np.argmax(word_cosine_scores)
    best_word = doc_words[best_word_idx]

    start_pos = document.lower().find(best_word)
    end_pos = start_pos + len(best_word) if start_pos != -1 else 0
    positions_str = f"{start_pos}-{end_pos}" if start_pos != -1 else "0-0"

    return {
        "document": document,
        "distance": float(cosine_distance),
        "positions": positions_str,
        "matched_word": best_word
    }


# === –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ===
documents = [
    '–¥–æ—á–µ–Ω—å–∫–∞ —Ç–≤–æ—è —Å–æ–≤—Å–µ–º –±–æ–ª—å—à–∞—è —Å—Ç–∞–ª–∞',
    '–≤ –ª–µ—Å—É —Ä–∞—Å—Ç—ë—Ç –≤—ã—Å–æ–∫–æ–µ –¥–µ—Ä–µ–≤–æ',
    '–ø–æ—Ä—Ç—Ñ–µ–ª—å –ª–µ–∂–∏—Ç –≤ —à–∫–∞—Ñ—É –¥–æ–º–∞',
    '–∏–Ω—Å—Ç–∏—Ç—É—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ü–µ–Ω—Ç—Ä–µ –≥–æ—Ä–æ–¥–∞',
    '–≤—Å—è –¥–æ—Ä–æ–≥–∞ –∑–∞–±–∏—Ç–∞ –¥–µ—Ä–µ–≤—å—è–º–∏ –∏ —Ü–≤–µ—Ç–∞–º–∏',
    '–≤ —Å–ª–µ–¥—É—é—â–µ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ —è —Å–æ–±–∏—Ä–∞—é—Å—å –≤ –ø–∏—Ç–µ—Ä',
    '—É –º–µ–Ω—è —Å–ª–æ–º–∞–ª–∞—Å—å —Å—Ç–∏—Ä–∞–ª–∫–∞ –ø—Ä–∏–∫–∏–Ω—å',
    '—Å–∞–¥–∏—Å—å –≤ –º–∞—à–∏–Ω—É –∏ –ø–æ–µ—Ö–∞–ª–∏ —É–∂–µ',
    '—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç —Ä–µ–º–æ–Ω—Ç —Å—Ç–∏—Ä–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω—ã',
    '—Ç—ã –≤–æ–∑—å–º–∏ –∫–æ—Ä–∑–∏–Ω—É –ø—Ä–µ–∂–¥–µ —á–µ–º –Ω–∞–±—Ä–∞—Ç—å –ø—Ä–æ–¥—É–∫—Ç—ã',
    '–µ–≥–æ —Å–µ–≥–æ–¥–Ω—è —É—Ç—Ä–æ–º –æ—Ç–≤–µ–∑–ª–∏ –≤ –±–ª–∏–∂–∞–π—à–∏–π –≥–æ—Å–ø–∏—Ç–∞–ª—å'
]

queries = [
    '–¥–æ—á—å',
    '–¥–µ—Ä–µ–≤–æ',
    '–ø–æ—Ä—Ç—Ñ–µ–ª—å',
    '–∏–Ω—Å—Ç–∏—Ç—É—Ç',
    '–¥–µ—Ä–µ–≤–æ',
    '—Å–∞–Ω–∫—Ç –ø–µ—Ç–µ—Ä–±—É—Ä–≥',
    '—Å—Ç–∏—Ä–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞',
    '–∞–≤—Ç–æ–º–æ–±–∏–ª—å',
    '–∞–≤—Ç–æ–º–æ–±–∏–ª—å',
    '–∑–≤–æ–Ω–∏—Ç—å',
    '–±–æ–ª—å–Ω–∏—Ü–∞'
]

ground_truth_words = [
    '–¥–æ—á–µ–Ω—å–∫–∞',
    '–¥–µ—Ä–µ–≤–æ',
    '–ø–æ—Ä—Ç—Ñ–µ–ª—å',
    '–∏–Ω—Å—Ç–∏—Ç—É—Ç',
    '–¥–µ—Ä–µ–≤—å—è–º–∏',
    '–ø–∏—Ç–µ—Ä',
    '—Å—Ç–∏—Ä–∞–ª–∫–∞',
    '–º–∞—à–∏–Ω—É',
    '–º–∞—à–∏–Ω—ã',
    '–∫–æ—Ä–∑–∏–Ω—É',
    '–≥–æ—Å–ø–∏—Ç–∞–ª—å'
]

assert len(documents) == len(queries) == len(ground_truth_words), "‚ùå –î–ª–∏–Ω—ã —Å–ø–∏—Å–∫–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç"

# === –ü–æ–∏—Å–∫ + –º–µ—Ç—Ä–∏–∫–∏ ===
results = []
exact_match_count = 0
distances = []

for doc, query, expected_word in zip(documents, queries, ground_truth_words):
    result = semantic_search_in_document(doc, query, ST_model)

    if result["distance"] <= 0.5:
        matched_word = result["matched_word"]
        is_exact = matched_word.lower() == expected_word.lower()
        exact_match_count += int(is_exact)

        results.append({
            "–¥–æ–∫—É–º–µ–Ω—Ç": doc,
            "–∑–∞–ø—Ä–æ—Å": query,
            "–æ–∂–∏–¥–∞–ª–æ—Å—å": expected_word,
            "–Ω–∞–π–¥–µ–Ω–æ": matched_word,
            "–ø–æ–∑–∏—Ü–∏—è": result["positions"],
            "–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ_—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ": round(result["distance"], 4),
            "—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ": is_exact
        })
        distances.append(result["distance"])

# === –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ ===
coverage = len(results) / len(documents)
mean_distance = np.mean(distances) if distances else 1.0
accuracy = exact_match_count / len(results) if results else 0.0

metrics = {
    "total_documents": len(documents),
    "matched_documents": len(results),
    "coverage": round(coverage, 3),
    "mean_distance": round(mean_distance, 4),
    "exact_match_accuracy": round(accuracy, 3)
}

# === –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:")
for r in results:
    print(
        f"üìÑ: {r['–¥–æ–∫—É–º–µ–Ω—Ç']}\nüîé: {r['–∑–∞–ø—Ä–æ—Å']} ‚Üí {r['–Ω–∞–π–¥–µ–Ω–æ']} | –æ–∂–∏–¥–∞–ª–æ—Å—å: {r['–æ–∂–∏–¥–∞–ª–æ—Å—å']} | –ø–æ–∑–∏—Ü–∏—è: {r['–ø–æ–∑–∏—Ü–∏—è']} | —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {r['–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ_—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ']}\n"
    )

print("=== üìà –ú–µ—Ç—Ä–∏–∫–∏ ===")
for k, v in metrics.items():
    print(f"{k}: {v}")

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
os.makedirs("model", exist_ok=True)
joblib.dump(results, "model/search_results.pkl")
joblib.dump(metrics, "model/search_metrics.pkl")
ST_model.save("model/semantic_search_model")
